{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00951fe",
   "metadata": {},
   "source": [
    "The AI Build-from-Scratch Challenges ðŸ’¡\n",
    "1. Designing AI-Powered Emissions Anomaly Detection ðŸ“ˆ\n",
    "Scenario: We need a real-time pipeline to detect anomalies in diverse emissions data streams, considering seasonality and noise.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "Architect & Build: Design an end-to-end pipeline. Implement a basic LSTM Autoencoder in PyTorch or TensorFlow. The provided snippet has a flawed structure. Fix the model architecture (ensure input/output shapes match, layers are appropriate) and write a basic training loop (using simulated data) that calculates reconstruction error.\n",
    "Refine: Explain your choices and how you'd handle preprocessing (scaling, detrending) and feature extraction for real-world deployment.\n",
    "Code Sample (PyTorch LSTM AE - Needs Fixing!):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f911aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM_AE(nn.Module):\n",
    "    def __init__(self, input_dim=5, hidden_dim=32, latent_dim=16, num_layers=1):\n",
    "        super(LSTM_AE, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc_enc = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        self.fc_dec = nn.Linear(latent_dim, hidden_dim)\n",
    "        \n",
    "        self.decoder = nn.LSTM(hidden_dim, input_dim, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # PROBLEM 3: How to get the *final* hidden state to feed the decoder?\n",
    "        # PROBLEM 4: Decoder needs an initial input. How to provide it?\n",
    "        _, (hidden, _) = self.encoder(x)\n",
    "        outputs, _ = self.decoder(hidden.permute(1, 0, 2)) # This won't work.\n",
    "        return outputs\n",
    "\n",
    "# PROBLEM 5: Needs a training loop: Generate sample data, run model, calc MSE loss.\n",
    "# Our Take: LSTMs are great for sequences, but building a working Autoencoder\n",
    "# requires careful architecture. Fix this model and show us a basic training setup.\n",
    "\n",
    "def forward(self, x):\n",
    "    # Encoder\n",
    "    _, (hidden, _) = self.encoder(x)  # hidden shape: (num_layers, batch, hidden_dim)\n",
    "    last_hidden = hidden[-1]  # Take the last layer's hidden state\n",
    "    \n",
    "    # Latent space\n",
    "    latent = self.enc_fc(last_hidden)  # (batch, latent_dim)\n",
    "    \n",
    "    # Prepare for decoder - expand latent to sequence\n",
    "    latent = self.dec_fc(latent)  # (batch, hidden_dim)\n",
    "    latent = latent.unsqueeze(1).repeat(1, x.size(1), 1)  # (batch, seq_len, hidden_dim)\n",
    "    \n",
    "    # Decoder\n",
    "    reconstructed, _ = self.decoder(latent)  # (batch, seq_len, input_dim)\n",
    "    \n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4738e7",
   "metadata": {},
   "source": [
    "The encoder compresses the input sequence into a hidden representation.\n",
    "I made a fully connected layer to compress  the final hidden state into a latent vector (bottleneck).\n",
    "then a fully conntected layer to expand it back to the decoderâ€™s hidden dimension.\n",
    "finally,  the decoder reconstructs the sequence from  a repeated version of this expanded latent vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e92f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
