{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00951fe",
   "metadata": {},
   "source": [
    "The AI Build-from-Scratch Challenges üí°\n",
    "1. Designing AI-Powered Emissions Anomaly Detection üìà\n",
    "Scenario: We need a real-time pipeline to detect anomalies in diverse emissions data streams, considering seasonality and noise.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "Architect & Build: Design an end-to-end pipeline. Implement a basic LSTM Autoencoder in PyTorch or TensorFlow. The provided snippet has a flawed structure. Fix the model architecture (ensure input/output shapes match, layers are appropriate) and write a basic training loop (using simulated data) that calculates reconstruction error.\n",
    "Refine: Explain your choices and how you'd handle preprocessing (scaling, detrending) and feature extraction for real-world deployment.\n",
    "Code Sample (PyTorch LSTM AE - Needs Fixing!):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f911aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM_AE(nn.Module):\n",
    "    def __init__(self, input_dim=5, hidden_dim=32, latent_dim=16, num_layers=1):\n",
    "        super(LSTM_AE, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc_enc = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        self.fc_dec = nn.Linear(latent_dim, hidden_dim)\n",
    "        \n",
    "        self.decoder = nn.LSTM(hidden_dim, input_dim, num_layers, batch_first=True)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1) # original sequence length\n",
    "\n",
    "        # 1 Encoder: we process the input x ‚Üí hidden states\n",
    "        _, (hidden, _) = self.encoder(x)\n",
    "\n",
    "        # 2 Latent space: take last layer‚Äôs hidden state and project into latent_dim\n",
    "        latent = self.fc_enc(hidden[-1])  # ‚Üí (batch, latent_dim)\n",
    "        \n",
    "        # 3. Decoder prep: expand latent back, add time axis, repeat\n",
    "        decoder_hidden = self.fc_dec(latent)\n",
    "        decoder_input = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        \n",
    "        # 4 Decoder: we decode to reconstruct the sequence from repeated latent representations\n",
    "        reconstructed, _ = self.decoder(decoder_input)  # ‚Üí (batch, seq_len, input_dim)\n",
    "        \n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4738e7",
   "metadata": {},
   "source": [
    "The encoder compresses the input sequence into a hidden representation.\n",
    "\n",
    "I made a fully connected layer to compress  the final hidden state into a latent vector (bottleneck).\n",
    "\n",
    "then a fully conntected layer to expand it back to the decoder‚Äôs hidden dimension.\n",
    "\n",
    "finally,  the decoder reconstructs the sequence from  a repeated version of this expanded latent vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8f47a8",
   "metadata": {},
   "source": [
    "for problem3, the *final* hidden state is got from last_hidden = hidden[-1]. This last_hidden tensor, with shape (batch_size, hidden_dim) is then passed through a linear layer to form the latent bottleneck : latent = self.fc_enc(last_hidden) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baae739",
   "metadata": {},
   "source": [
    "for problem4, The decoder expects a full sequence of inputs, not just one vector. We convert our latent vector into a ‚Äúfake‚Äù sequence by expanding the latent vector back to the decoder's hidden size (step3) andthen repeat it across all steps and later feed it into the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47f0b01",
   "metadata": {},
   "source": [
    "PROBLEM 5: Needs a training loop\n",
    "  1) Generate synthetic emissions-like data\n",
    "  2) Wrap it in a DataLoader\n",
    "  3) Initialize model, MSE loss, and optimizer\n",
    "  4) For each epoch:\n",
    "       a) Forward pass ‚Üí reconstruction\n",
    "       b) Compute MSE loss between recon and input\n",
    "       c) Backward + optimizer.step()\n",
    "       d) Optionally print/log average loss\n",
    "\n",
    "Our Take: LSTM Autoencoders require both a proper architecture (encoder, bottleneck, decoder) and a straightforward training loop that measures reconstruction error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eaf9343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup:\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b89288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Generate data\n",
    "def generate_data(n_seq=1000, seq_len=50, feat=5):\n",
    "    t = torch.linspace(0, 2*torch.pi, seq_len)\n",
    "    data = torch.sin(t).unsqueeze(-1).repeat(1, feat)  # shape (seq_len, feat)\n",
    "    data = data + 0.1 * torch.randn(n_seq, seq_len, feat)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96e999db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(generate_data())\n",
    "loader  = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f0a0360",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Model, loss, optimizer\n",
    "learning_rate = 0.001\n",
    "model     = LSTM_AE(input_dim=5, hidden_dim=32, latent_dim=16)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50f0749a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 ‚Äî Avg Recon MSE: 0.5060\n",
      "Epoch  1 ‚Äî Avg Recon MSE: 0.4999\n",
      "Epoch  2 ‚Äî Avg Recon MSE: 0.4997\n",
      "Epoch  3 ‚Äî Avg Recon MSE: 0.4998\n",
      "Epoch  4 ‚Äî Avg Recon MSE: 0.4997\n",
      "Epoch  5 ‚Äî Avg Recon MSE: 0.4999\n",
      "Epoch  6 ‚Äî Avg Recon MSE: 0.4999\n",
      "Epoch  7 ‚Äî Avg Recon MSE: 0.4999\n",
      "Epoch  8 ‚Äî Avg Recon MSE: 0.4998\n",
      "Epoch  9 ‚Äî Avg Recon MSE: 0.4997\n",
      "Epoch 10 ‚Äî Avg Recon MSE: 0.4998\n",
      "Epoch 11 ‚Äî Avg Recon MSE: 0.4997\n",
      "Epoch 12 ‚Äî Avg Recon MSE: 0.4999\n",
      "Epoch 13 ‚Äî Avg Recon MSE: 0.4997\n",
      "Epoch 14 ‚Äî Avg Recon MSE: 0.4997\n",
      "Epoch 15 ‚Äî Avg Recon MSE: 0.4998\n",
      "Epoch 16 ‚Äî Avg Recon MSE: 0.4998\n",
      "Epoch 17 ‚Äî Avg Recon MSE: 0.4999\n",
      "Epoch 18 ‚Äî Avg Recon MSE: 0.4998\n",
      "Epoch 19 ‚Äî Avg Recon MSE: 0.4998\n",
      "Epoch 20 ‚Äî Avg Recon MSE: 0.5000\n",
      "Epoch 21 ‚Äî Avg Recon MSE: 0.4997\n",
      "Epoch 22 ‚Äî Avg Recon MSE: 0.4997\n",
      "Epoch 23 ‚Äî Avg Recon MSE: 0.4998\n",
      "Epoch 24 ‚Äî Avg Recon MSE: 0.4998\n",
      "Epoch 25 ‚Äî Avg Recon MSE: 0.4998\n",
      "Epoch 26 ‚Äî Avg Recon MSE: 0.4998\n",
      "Epoch 27 ‚Äî Avg Recon MSE: 0.4998\n",
      "Epoch 28 ‚Äî Avg Recon MSE: 0.4998\n",
      "Epoch 29 ‚Äî Avg Recon MSE: 0.4998\n",
      "Epoch 30 ‚Äî Avg Recon MSE: 0.4999\n",
      "Epoch 31 ‚Äî Avg Recon MSE: 0.4998\n",
      "Epoch 32 ‚Äî Avg Recon MSE: 0.4998\n",
      "Epoch 33 ‚Äî Avg Recon MSE: 0.4997\n",
      "Epoch 34 ‚Äî Avg Recon MSE: 0.4997\n",
      "Epoch 35 ‚Äî Avg Recon MSE: 0.4998\n",
      "Epoch 36 ‚Äî Avg Recon MSE: 0.4998\n",
      "Epoch 37 ‚Äî Avg Recon MSE: 0.4997\n",
      "Epoch 38 ‚Äî Avg Recon MSE: 0.4999\n",
      "Epoch 39 ‚Äî Avg Recon MSE: 0.4998\n",
      "Epoch 40 ‚Äî Avg Recon MSE: 0.4999\n",
      "Epoch 41 ‚Äî Avg Recon MSE: 0.4997\n",
      "Epoch 42 ‚Äî Avg Recon MSE: 0.4998\n",
      "Epoch 43 ‚Äî Avg Recon MSE: 0.4998\n",
      "Epoch 44 ‚Äî Avg Recon MSE: 0.4998\n",
      "Epoch 45 ‚Äî Avg Recon MSE: 0.4997\n",
      "Epoch 46 ‚Äî Avg Recon MSE: 0.4997\n",
      "Epoch 47 ‚Äî Avg Recon MSE: 0.4934\n",
      "Epoch 48 ‚Äî Avg Recon MSE: 0.4707\n",
      "Epoch 49 ‚Äî Avg Recon MSE: 0.3882\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3) Training loop\n",
    "num_epochs = 50 \n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for x_batch, in loader:\n",
    "        recon = model(x_batch)\n",
    "        loss  = criterion(recon, x_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"Epoch {epoch:2d} ‚Äî Avg Recon MSE: {avg_loss:.4f}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3416ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Save the model\n",
    "torch.save(model.state_dict(), 'lstm_ae.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
